{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b7WiwcJBjSYh"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Bidirectional, Dropout, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tensorflow.keras.callbacks import Callback, CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zns-KalI-BwJ",
        "outputId": "a6a6ba74-b78b-494a-c687-998aed572b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/GDP\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "folder structure:\n",
        "\n",
        "GDP\n",
        "  test\n",
        "  train\n",
        "  weights\n",
        "\n",
        "\"\"\"\n",
        "# Check which directory are you in:\n",
        "\n",
        "# your working directory should be in the main folder where \"weights\" folder is in.\n",
        "\n",
        "# print(os.getcwd())\n",
        "\n",
        "# %cd /content/drive/MyDrive/GDP \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAypd8EUKJGJ"
      },
      "source": [
        "### Custom Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ijBhsZNyKFj1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import Callback, CSVLogger\n",
        "def compute_test(model, X_test_reshaped, y_test):\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "    # Convert the predicted probabilities to class labels\n",
        "    y_pred_classes = y_pred.argmax(axis=-1)\n",
        "\n",
        "    # Compute test accuracy\n",
        "    correct=0\n",
        "    for i in range(len(y_pred_classes)):\n",
        "        if y_pred_classes[i]==y_test[i]:\n",
        "            correct+=1\n",
        "    test_accuracy=correct/len(y_pred_classes)\n",
        "\n",
        "    return test_accuracy, y_pred_classes\n",
        "\n",
        "class SaveWeightsJSON(Callback):\n",
        "    def __init__(self, X_test_reshaped, y_test, param_idx, sup_hyper_param_list, model_type, num_frames, num_features, weights_path):\n",
        "        super(SaveWeightsJSON, self).__init__()\n",
        "        self.start_time = time.time()\n",
        "        self.model_name=f\"model{param_idx}\"\n",
        "        self.weights_path = weights_path\n",
        "        self.X_test = X_test_reshaped\n",
        "        self.y_test = y_test\n",
        "        \n",
        "        self.json_data={\n",
        "                \"model_name\": self.model_name,\n",
        "                \"model_type\": model_type,\n",
        "                \"num_frames\": num_frames,\n",
        "                \"num_features\": num_features, \n",
        "                \"num_layers\": sup_hyper_param_list[0],\n",
        "                \"dropout_rate\": sup_hyper_param_list[1],\n",
        "                \"optimizer\": sup_hyper_param_list[2],\n",
        "                \"batch_size\": sup_hyper_param_list[3],\n",
        "                \"learning_rate\": sup_hyper_param_list[4],\n",
        "                \"best_epoch\": 0,\n",
        "                \"best_train_acc\": 0.0,\n",
        "                \"best_test_acc\": 0.0,\n",
        "                \"training_time\": 0.0\n",
        "        }\n",
        "\n",
        "        self.best_epoch = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_train_accuracy = 0.0\n",
        "        self.best_test_accuracy = 0.0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = logs.get('val_loss')\n",
        "        test_acc, _ = compute_test(self.model, self.X_test, self.y_test)\n",
        "\n",
        "        if val_loss <= self.best_val_loss:\n",
        "            if test_acc > self.best_test_accuracy:\n",
        "                self.best_val_loss = val_loss\n",
        "\n",
        "                self.best_epoch = epoch + 1\n",
        "                self.best_train_accuracy = logs.get('accuracy')\n",
        "                self.best_test_accuracy = test_acc\n",
        "                weights_file_path = os.path.join(self.weights_path, f\"{self.model_name}.h5\")\n",
        "                self.model.save_weights(weights_file_path, overwrite=True)\n",
        "                                \n",
        "    def on_train_end(self, logs=None):\n",
        "        self.json_data['best_epoch'] = self.best_epoch\n",
        "        self.json_data['best_train_acc'] = self.best_train_accuracy\n",
        "        self.json_data['best_test_acc'] = self.best_test_accuracy\n",
        "        json_file_path = os.path.join(self.weights_path, f\"{self.model_name}.json\")\n",
        "        self.json_data['training_time'] = time.time() - self.start_time\n",
        "        with open(json_file_path, 'w') as f:\n",
        "            json.dump(self.json_data, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60wC8CjoKbUg"
      },
      "source": [
        "### Main Modeling Function (including reading data & preprocessing data & create and fit the desired model to the data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SIWH-BxoqIiy"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Bidirectional, Dropout, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_data(split_path):\n",
        "  X = []\n",
        "  Y = []\n",
        "  seqs_list=os.listdir(split_path)\n",
        "  random.shuffle(seqs_list)\n",
        "  for seq in seqs_list:\n",
        "    seq_file_path=os.path.join(split_path,seq)\n",
        "    with open(seq_file_path, \"r\") as f:\n",
        "      data = json.load(f)\n",
        "      X.append(data['features'])\n",
        "      Y.append(data['class'])\n",
        "  X = np.array(X)\n",
        "  Y = np.array(Y)\n",
        "\n",
        "  return X,Y\n",
        "\n",
        "def opt_function(optimizer,learning_rate):\n",
        "\n",
        "  if optimizer == 'adam':\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "  else:\n",
        "    opt = SGD(learning_rate=learning_rate)\n",
        "\n",
        "  return opt\n",
        "\n",
        "def compile_model(sup_hyper_param_list,lstm_units,model_type, num_frame, num_features):\n",
        "\n",
        "  num_kpsORjnts=int(num_features/2)\n",
        "  num_layer= sup_hyper_param_list[0]\n",
        "  dropout_rate= sup_hyper_param_list[1]\n",
        "  optimizer= sup_hyper_param_list[2]\n",
        "  #batch_size= sup_hyper_param_list[3]\n",
        "  learning_rate= sup_hyper_param_list[4]\n",
        "\n",
        "  opt = opt_function(optimizer,learning_rate)\n",
        "\n",
        "  num_layer_list = list(range(0,num_layer))\n",
        "  num_layer_list.reverse()\n",
        "\n",
        "\n",
        "# Define paramters used in sup_hyper_param_list\n",
        "\n",
        "  if model_type == 'LSTM':\n",
        "\n",
        "    model = Sequential ()\n",
        "    model.add(LSTM(units = (lstm_units)*(2**num_layer), return_sequences=True, input_shape=(num_frame,num_features)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    for s in num_layer_list:\n",
        "\n",
        "      if s == 0:\n",
        "        model.add(LSTM(units = (lstm_units)*(2**s), return_sequences=False))\n",
        "      else:\n",
        "\n",
        "        model.add(LSTM(units = (lstm_units)*(2**s), return_sequences=True))\n",
        "      model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    #model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "  elif model_type == 'CNN_LSTM':\n",
        "    \n",
        "\n",
        "    model = Sequential ()\n",
        "    model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=(num_frame,num_kpsORjnts,2)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Reshape((model.output_shape[1],model.output_shape[2]*model.output_shape[3])))\n",
        "    model.add(LSTM(units = (lstm_units)*(2**num_layer), return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    for s in num_layer_list:\n",
        "      \n",
        "      if s == 0:\n",
        "        model.add(LSTM(units = (lstm_units)*(2**s), return_sequences=False))\n",
        "      else:\n",
        "\n",
        "        model.add(LSTM(units = (lstm_units)*(2**s), return_sequences=True))\n",
        "      model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    #model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "  elif model_type == 'BiLSTM':\n",
        "\n",
        "    model = Sequential ()\n",
        "    model.add(Bidirectional(LSTM(units = (lstm_units)*(2**num_layer),return_sequences=True), input_shape=(num_frame,num_features)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    for s in num_layer_list:\n",
        "\n",
        "      if s == 0:\n",
        "        model.add(Bidirectional(LSTM(units = (lstm_units)*(2**s),return_sequences=False)))\n",
        "      else:\n",
        "\n",
        "        model.add(Bidirectional(LSTM(units = (lstm_units)*(2**s),return_sequences=True)))\n",
        "      model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    #model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "  elif model_type == 'CNN_BiLSTM':\n",
        "  \n",
        "    model = Sequential ()\n",
        "    model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=(num_frame,num_kpsORjnts,2)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Reshape((model.output_shape[1],model.output_shape[2]*model.output_shape[3])))\n",
        "    model.add(Bidirectional(LSTM(units = (lstm_units)*(2**num_layer),return_sequences=True), input_shape=(num_frame,num_features)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    for s in num_layer_list:\n",
        "\n",
        "      if s == 0:\n",
        "        model.add(Bidirectional(LSTM(units = (lstm_units)*(2**s),return_sequences=False)))\n",
        "      else:\n",
        "\n",
        "        model.add(Bidirectional(LSTM(units = (lstm_units)*(2**s),return_sequences=True)))\n",
        "      model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "  else:\n",
        "    print('Invalid model name. You can only use these models: \"CNN\",\"CNN_LSTM\",\"BiLSTM\",\"CNN_BiLSTM\"')\n",
        "\n",
        "\n",
        "def run_models(hyper_param_list,epochs,lstm_units,split_train_path,split_test_path,weights_path,model_type):\n",
        "\n",
        "    print(\"Number of configurations to run: \", len(hyper_param_list))\n",
        "    X_train, y_train = load_data(split_train_path)\n",
        "    num_json,num_frames,num_features = X_train.shape \n",
        "\n",
        "    X_test, y_test = load_data(split_test_path)\n",
        "    num_json_test = X_test.shape[0]\n",
        "\n",
        "    num_classes = 3\n",
        "    y_train_cat=to_categorical(y_train,num_classes)\n",
        "    y_test_cat=to_categorical(y_test,num_classes)\n",
        "\n",
        "    num_kpsORjnts=int(num_features/2)\n",
        "\n",
        "    if model_type == 'CNN_LSTM' or model_type == 'CNN_BiLSTM':\n",
        "\n",
        "      X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], num_kpsORjnts, 2))\n",
        "      X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], num_kpsORjnts, 2))\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    for param_idx, sup_hyper_param_list in enumerate(hyper_param_list): \n",
        "      print(sup_hyper_param_list)\n",
        "      save_weights_json = SaveWeightsJSON(X_test_reshaped=X_test, y_test=y_test, param_idx=param_idx,sup_hyper_param_list=sup_hyper_param_list,\n",
        "                                        model_type=model_type, num_frames=num_frames, num_features=num_features, weights_path=weights_path)\n",
        "      \n",
        "      log_file_path = os.path.join(weights_path, f\"model{param_idx}.log\")\n",
        "      save_log = CSVLogger(log_file_path)\n",
        "\n",
        "      model = compile_model(sup_hyper_param_list=sup_hyper_param_list,lstm_units=lstm_units,\n",
        "                            model_type=model_type, num_frame = num_frames, num_features = num_features)\n",
        "      \n",
        "      model_fit = model.fit(X_train, y_train_cat, epochs=epochs, validation_split = 0.2, batch_size=sup_hyper_param_list[3],\n",
        "                            callbacks=[save_weights_json,save_log])\n",
        "      \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run and Fit Model with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "aycjZL09BOtc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COww-Hc4043l",
        "outputId": "dbfc8c31-8890-4d3c-e154-d6f7bcecaaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of configurations to run:  2\n",
            "(1, 0.1, 'adam', 64, 0.01)\n",
            "Epoch 1/2\n",
            "34/34 [==============================] - 1s 3ms/step\n",
            "53/53 [==============================] - 11s 46ms/step - loss: 0.8020 - accuracy: 0.6730 - val_loss: 0.7338 - val_accuracy: 0.7180\n",
            "Epoch 2/2\n",
            "34/34 [==============================] - 0s 6ms/step\n",
            "53/53 [==============================] - 1s 26ms/step - loss: 0.7639 - accuracy: 0.6869 - val_loss: 0.7123 - val_accuracy: 0.7085\n",
            "(2, 0.1, 'adam', 64, 0.01)\n",
            "Epoch 1/2\n",
            "34/34 [==============================] - 1s 3ms/step\n",
            "53/53 [==============================] - 8s 49ms/step - loss: 0.8063 - accuracy: 0.6611 - val_loss: 0.7677 - val_accuracy: 0.6896\n",
            "Epoch 2/2\n",
            "34/34 [==============================] - 0s 3ms/step\n",
            "53/53 [==============================] - 1s 13ms/step - loss: 0.7533 - accuracy: 0.6851 - val_loss: 0.7266 - val_accuracy: 0.7050\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\"\"\" Running the desired model with several parameters using run_models():\n",
        "\n",
        "hyper_param_list  = DO NOT CHANGE. Keep as it is in the function call.\n",
        "\n",
        "epochs = Number of epochs you would like to try for each of the model combination\n",
        "\n",
        "lstm_units = LSTM layer units (recommendation: do not increase it too much, start with 16,32, 64)\n",
        "\n",
        "split_train_path = Path to train data\n",
        "\n",
        "split_train_path = Path to test data\n",
        "\n",
        "weights_path = Path to weights folder which all files regarding to model will be created\n",
        "\n",
        "model_type = Which model is you want to try? \n",
        "\n",
        "    options for model_type are:\n",
        "              LSTM\n",
        "              CNN_LSTM\n",
        "              BiLSTM\n",
        "              CNN_BiLSTM\n",
        "\n",
        "\"\"\"\n",
        "# You CAN change param_grid depending on which parameter combinations you would like to try in the model:\n",
        "\n",
        "param_grid = {\n",
        "              'dropout_rate': [0.1],\n",
        "              'batch_size': [64],\n",
        "              'num_layer': [1,2],\n",
        "              'optimizer': ['adam'],\n",
        "              'learning_rate': [0.01]\n",
        "              }\n",
        "\n",
        "# Do not change hyper_param_list\n",
        "\n",
        "hyper_param_list=[(num_layer, dropout_rate, optimizer, batch_size, learning_rate)\n",
        "                  for num_layer in param_grid['num_layer']\n",
        "                  for dropout_rate in param_grid['dropout_rate']\n",
        "                  for optimizer in param_grid['optimizer']\n",
        "                  for batch_size in param_grid['batch_size']\n",
        "                  for learning_rate in param_grid['learning_rate']\n",
        "                  ]\n",
        "\n",
        "\n",
        "\"\"\" Running the desired model with several parameters using run_models():\n",
        "\n",
        "hyper_param_list  = DO NOT CHANGE. Keep as it is in the function call.\n",
        "\n",
        "epochs = Number of epochs you would like to try for each of the model combination\n",
        "\n",
        "lstm_units = LSTM layer units (recommendation: do not increase it too much, start with 16,32, 64)\n",
        "\n",
        "split_train_path = Path to train data\n",
        "\n",
        "split_train_path = Path to test data\n",
        "\n",
        "weights_path = Path to weights folder which all files regarding to model will be created\n",
        "\n",
        "model_type = Which model is you want to try? \n",
        "\n",
        "    options for model_type are:\n",
        "              LSTM\n",
        "              CNN_LSTM\n",
        "              BiLSTM\n",
        "              CNN_BiLSTM\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "split_train_path = r\"C:\\Users\\Ivan Lopez\\Desktop\\GDP\\split_seqs\\run1_a+p_dist_1_0.5\\train\"\n",
        "split_test_path = r\"C:\\Users\\Ivan Lopez\\Desktop\\GDP\\split_seqs\\run1_a+p_dist_1_0.5\\test\"\n",
        "weights_path = r\"C:\\Users\\Ivan Lopez\\Desktop\\GDP\\weights\"\n",
        "\n",
        "# --- Irem - Colab Version  ---\n",
        "#split_train_path=\"/content/drive/MyDrive/GDP/train\"\n",
        "#split_test_path=\"/content/drive/MyDrive/GDP/test\"\n",
        "#weights_path=\"/content/drive/MyDrive/GDP/weights\"\n",
        "\n",
        "# Run a desired model using the belove line:\n",
        "\n",
        "run_models(hyper_param_list=hyper_param_list, epochs=2, lstm_units=16,\n",
        "            split_train_path=split_train_path,split_test_path=split_test_path,\n",
        "            weights_path=weights_path, model_type=\"LSTM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the best model path (highest test accuracy) among all best combinations"
      ],
      "metadata": {
        "id": "4hxNAMej_fGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "WEIGHTS_DIRECTORY_PATH = 'weights/'\n",
        "\n",
        "# Attribute based on which best model weights are to be selected\n",
        "SELECTION_KEY = \"best_test_acc\"\n",
        "\n",
        "# Min or Max value of SELECTION_KEY to decide the best model\n",
        "SELECT_MAX = True\n",
        "\n",
        "def read_all_json_files(weights_directory):\n",
        "    weights_path = weights_directory\n",
        "    json_files = glob(os.path.join(weights_path,\"**/*.json\"), recursive=True)\n",
        "    data = []\n",
        "    for file in json_files:\n",
        "        with open(file, 'r') as f:\n",
        "            json_data = json.load(f)\n",
        "            json_data['file_path'] = os.path.abspath(file)\n",
        "            data.append(json_data)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def get_best_model_weights_path(weights_directory, selection_criteria, select_max = True):\n",
        "    df = read_all_json_files(weights_directory)\n",
        "    if select_max:\n",
        "        best = df.loc[df[selection_criteria].idxmax()]\n",
        "    else:\n",
        "        best = df.loc[df[selection_criteria].idxmin()]\n",
        "    best_wt = os.path.splitext(best['file_path'])[0] + '.h5'\n",
        "    print('Best model path among all combinations is:')\n",
        "    return best_wt\n",
        "\n",
        "# Run belove line to get the path of the best model (highest test accuracy) among all of the model combinations in \"weights\" folder:\n",
        "\n",
        "best_model_wt_path = get_best_model_weights_path(WEIGHTS_DIRECTORY_PATH, SELECTION_KEY, SELECT_MAX)\n",
        "best_model_wt_path"
      ],
      "metadata": {
        "id": "yq6es9C6_9lD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}